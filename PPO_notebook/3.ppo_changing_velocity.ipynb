{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 64\n",
    "env_name = \"BikeLQR_4states-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_size, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_size, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    #clear_output(False)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('update %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False, init_state=None, nr_steps=int(1e9)):\n",
    "    if type(init_state) == np.ndarray:\n",
    "        state = env.reset(init_state=init_state)\n",
    "    else:\n",
    "        state = env.reset()\n",
    "        \n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for i in range(nr_steps):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        \n",
    "        if vis:\n",
    "            time.sleep(0.1)\n",
    "            env.render() \n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def get_phi_sequence(state_0):\n",
    "    state = state_0.copy()\n",
    "    phi_sequence = [state[0]]\n",
    "    delta_sequence = []\n",
    "    v_sequence = [state[2]]\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        phi_sequence.append(next_state[0])\n",
    "        delta_sequence.append(action)\n",
    "        v_sequence.append(next_state[2])\n",
    "        state = next_state.copy()\n",
    "    return phi_sequence, delta_sequence, v_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 32\n",
    "lr               = 1e-5\n",
    "#lr               = 1e-6\n",
    "num_steps        = 60\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 10000000\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ppo_updates = 40000\n",
    "ppo_update_idx  = 0\n",
    "test_rewards = []\n",
    "highest_test_reward = -np.inf\n",
    "#highest_test_reward = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('saved_networks/ppo_network_IC_uni_pm5_v_0dot5_to_10_uni_eplen100_lr9e-6_brute_3x128_nodes_ver3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAE/CAYAAABLrsQiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH61JREFUeJzt3H28XFV97/HPFw4QCQ1JIAEkBKJGMKEKdSQioIiJBmubhIf7wgeIFptLwVsVqYDUW1DbBrRFq15LruiNojyIIihFLnAJqC8bPYFEcjyGxPAUEiBpeUpStDG/+8dap+wMc85MMpmcnKzv+/Xar7P3WmvvWWtmznz304wiAjMzK9dug90BMzMbXA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQh2cpLeL+kng92PXZmkBZI+ONj9MBssDoJdiKRLJV3TxvqzJS2S9JykVZKukNRVqR8t6SZJGyQ9Iuk926fnQ5ukeZKWSdos6f11dWfkumclPSVpvqQRlfoFkl6QtD5Py/p5jK9LCkmv6qf+hMo2+qaQdGquf7+k39fVn5jrxkq6VtLq3M+fSppSt/335Nd8g6TvSxpdqat/3N9L+mKumySpW9LTebpT0qTKum+VdHd+3IcbjOtNkn4u6XlJv5R0fKXuxPycVx97doNtTMzP8TWVMkm6RNKj+f1+Xd3rspekr+W6JySdX6kbcExDkYPAqvYGPgLsD0wB3gZcUKn/MvA74ADgvcBXJE3e2gfJ/4Q7/L1XDbXtbAlwLnBfg7qfAsdFxL7AK4Au4DN1bT4UEfvk6fD6DeQPv1cO1IGI+HFlG/sA7wLWAz+qNPtZtU1ELMjl+wC/AF4PjAbmA7dK2ic//mTgKuBM0mu/EfhflceuPu4BwH8A38nVq4HT8nb3B24Brqv0aQPwNeCvGox7dG7/WWAkcAXwA0mjKs1W141pfoOn58t5fFVn5fEcB7wceBnwxUr9pcBE4FDgrcDHJU1vcUxDT0R42s4TEMCrKsv/B/hMnj8RWAV8AlgHPAy8t9J2P9Ib6zng58CngZ9U6r8APJbrFwEn5PLppA/p/yR9ACzJ5fsCVwNrgMdJH0K7tziO84Ef5PnhefuvrtR/E5jb4rYWAH9L+mD8D+BVA/UNeAR4fZ5/X35OJ+XlDwLfz/PHAD8Dnsnb+RKwZ91rcR6wHHgol00Dfg08m9vfA3xwO7zuPwHeP0D9PsA3gH+pe176fWxScNwPvLb+fdWkL18Hvl5Zfn/1fdTC+s9Vnv+/A75dqXtlfi/8QYP1ZgMrAfUzlvOAjQ3qpgIP15W9C+ipK3sQOLv6v9RkHGcAN5A+2K+plN8I/FVl+U3AC8Deeflx4O2V+k8D123NmIbS5COCwXEgaU/iYNI/zjxJfXuCXya9IQ8C/ixPVb8AjiLtjXwb+I6kYRHxI9I/7PWR9oxel9vPBzaRPniPBt5O+iBtxZuBnjz/auD3EfFgpX4JsDVHBGcCc4A/IH3QD9S3e0j/6H39WAm8pbJ8T57/PfBR0vN5LOko5ty6x51JOsKZJGl/4LvAX+d1fkPaKwRA0nhJz0gavxXjGpCk4yU9CzwPnAp8vq7J30tal0/JnFhX91Hg3oj45VY83t6kPdb6veOj8+M8KOmT/R0hSToK2BNYkYsmk15rACLiN+Sdggarzwa+EflTsrLNZ0jv6y+S3qctDSVP9WVHVpbHSnpS0kOSrpQ0vPKYI4BPAR9rYdsC9gIm5iOOl1MZMw3e69s4pp3TYCfRrjjR/IhgEzC8Un8D8Elgd9Ie/RGVur9jgD054GngdXn+Urbc6zkA+C3wskrZu4G7WxjDB0hHLvvn5ROAJ+ra/DmwoMXnZAHwqVb7BpwN3JLne0kBcV1efgT4o34e5yPATXWvxUmV5bOAf60sK49zRxwRHJxfo+pR1RRSMO5F+hB9HnhlrjuE9GG8b6P31QCPcybwEJW9ctJpqQmk08F/CPwKuLjBuiOAB6p1wF3AOXXtHgdOrCsbTwrmCf30azgppP+4QV2jI4L9SEd67wb2yM/PZuCqXH8gMCmPaQJwb19drv8CcGE//xsfJB1dHEY6Mr0lP7/H5uc9gGGV9tPq+9dsTENp8hHB4Hg6IjZUlh8h7YGMIR1qPlZX918kfUxSb7649gzpTbx/P49zKOkfaE3ey32GdK537ECdkzQTmAucHBHrcvF60odE1QjSB1erquNq1rd7gBMkHUgKyOuB4yQdRhrz4tzXV0v6Yb6g9xwpOOufj+rjvry6HOm/+TFaVHdhcquOGiLicdI5++sqZQsj4vmI+G2k89s/Bd6Zqz9PCs9nt+ZxaLBXHhErI+KhiNgcEQ+Q9pRPqxvby4AfkILy7ytVrb72Z5F2Wh5q1Kn8nv9n4BuSBnwP5vb/BswgnaJ8knT6805ScBMRT0TEr/KYHgI+3jemfFQzFbiyn81/DbiWtIPSA9ydy1fl8faNcaDxbvWYdlYOgs7YSLrw2ufAuvpR1UNY0p7UamAt6WjhkLo6IN0ZAlwI/DdgVESMJJ3n7jvErf8p2cdIe937R8TIPI2IiH5P5+QLYv8b+JP8gdHnQaBL0sRK2et48dRRK6r9G7BvEbGC9Dz+JenUyPPAE6RTSz+JiM15O18hne+fGBEjSNde6k8nVB93DZXnV5LY8vkeeABbXph8tNX1KroY+MJv8GL/3wZ8NofcE7nsZxrgbi1Jh5COOr/RpB/Vx0HSXsD3SXv6/72ubQ/pte5r+wrSEcyDde3O4qWno+rtRvrfOLhJu9TJiHsi4g0RMZp0pHM46dpZw+a8OKYTSXv7j+bn7gLgVEn35e1ujoi/iYjDImJcHuPjwOMR8TTpffK6yrYHeq9v1Zh2SoN9SLIrTqS9urmkPdnppIuj9aeGPkc6D3sC6c6JI3L99aQ9xr1Jh72ryKeGSHuKq0nBsifwP0mH4lNz/Tmk0xO7VfpyM+kQeQTpDftK4C399Psk4N+AN/dTfx1pL2o46bz6s8DkFp+TBdSdfmnWN9I1kOeAM/PyZ/Ny9SLfz/PzIOAIYBlbXlyvP023P2nP7hTSh/KH8+uxzaeG8msxLL/uf57nd8t17yWFuUhHQfcA38t1I4F35PZdue0G4PBcPza/1n1TAG+kcjqtQV8+QQrO+vKTgQPy/BHAUuBv8vIepCOB7wNdDdadnJ/3E/Jrfw11F05JF1s3UHcBmXRK5WjS/8II4J/ye3hYrt8tj/9k0tHvMLa82H907t8I0hHSTyt1J1ae20NIe/Vfz3V71z13nyNdIB6T60fn95tI/2dLgTmVbc/Nr9Wo/HytAaa3MqahOA16B3bFCaiR9h6eJ91Zcy0vvWvoEtJdQ4+SP+hy/RjghzS4ayi/8a7OdWtIh8IP82IQ7EcKgqeB+3LZvqS95lWkD+77gTP66ffdpA/F9ZXptkr96PxhsSH3+z2VuhOA9QM8Jwt4aRAM2DfSnmkAh+bld+XlKZU2byYdEawHfkw65dFvEOSy6aS92ZfcNUT6YFkPjN+K13tBfpzqdGKu+9s8vg357zxgv8pr/Yv8PnkG+Fdg2gCPUx9qtwGfqGvza/JdNXXlnyOdXtlAuvD+KWCPXPeWvO2Nda/9CZX135Nf8w2kAB9dt/2rgG82eNzTK6/PWuBfgNdW6k9s8NwtqNRfm1+nZ0k7SWMrdeeT9uI3ko4wv0iDO5ly20vZ8hrBq0k7DRtJAXR+Xfu9SKePnsvP2/mtjmkoTsoDsx0k3xVyTaTDUTOzQedrBGZmhWsrCCSdLqknf827VimfpvRTBQ/kvydV6t6dy38p6Uf5vu6+ny+4Q9Ly/HdUo8c0M7Ptq90jgqWki2731pWvI9118oekW9m+Cf/1Ff8vAG+NiNcCvwQ+lNe5CLgrIiaS7lu+qM2+7ZQiYoFPC5nZzqStIIiI3oh4yY9kRcT9EbE6L/YAw/LtaX3f5hueb9sbQbraDul+4b5bz+aTvg1qZmYd1qkf4ao6Fbg/In4LIOkvSN9c3ED6/ZfzcrsDImINQESsGcpfzjAzG0qaBoGkO3npF6IALomIm5usOxm4nPQbMkjaA/gL0j24K0m3e13MS3+NsVmf5pC+WMTw4cNff8QRR2zN6mZmRVi0aNG6iBjTrF3TIIiIqdvSAUnjgJuAsyL9SBWkH0ujb1nSDbx4LeBJSQflo4GDgKcG6NM80v3Y1Gq16O7u3pYumpnt0iQ90rxVh24flTQSuJX0w1U/rVQ9TvoFyL6Emkb6QTFIP/o0O8/PJn1pxczMOqzd20dnSVpF+sW+WyXdnqs+RPpp4U9KWpynsfkC8mXAvZJ+STpC6Pv51rnANEnLSQExt52+mZlZa4b8N4t9asjMrDFJiyKi1qydv1lsZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhWsrCCSdLqlH0mZJtUr5MZIW52mJpFmVuumSlklaIemiSvkESQslLZd0vaQ92+mbmZm1pt0jgqXAKcC9DcprEXEUMB24SlKXpN2BLwMnA5OAd0ualNe5HLgyIiYCTwNnt9k3MzNrQVtBEBG9EbGsQfnGiNiUF4cBkeePAVZExMqI+B1wHTBDkoCTgBtzu/nAzHb6ZmZmrenYNQJJUyT1AA8A5+RgOBh4rNJsVS7bD3imEh595WZm1mFdzRpIuhM4sEHVJRFxc3/rRcRCYLKk1wDzJd0GqFHTAcr769McYA7A+PHjB+i9mZk10zQIImJqOw8QEb2SNgBHkvb0D6lUjwNWA+uAkZK68lFBX3l/25wHzAOo1Wr9BoaZmTXXkVND+Q6grjx/KHA48DDwC2Birt8TOAO4JSICuBs4LW9iNtDv0YaZmW0/7d4+OkvSKuBY4FZJt+eq44ElkhYDNwHnRsS6vLf/IeB2oBe4ISJ68joXAudLWkG6ZnB1O30zM7PWKO2MD121Wi26u7sHuxtmZjsdSYsiotasnb9ZbGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFa6tIJB0uqQeSZsl1Srlx0hanKclkmbl8kMk3S2pN6/34co6oyXdIWl5/juqnb6ZmVlr2j0iWAqcAtzboLwWEUcB04GrJHUBm4CPRcRrgDcC50malNe5CLgrIiYCd+VlMzPrsLaCICJ6I2JZg/KNEbEpLw4DIpeviYj78vzzQC9wcG43A5if5+cDM9vpm5mZtaZj1wgkTZHUAzwAnFMJhr76w4CjgYW56ICIWAMpMICxneqbmZm9qKtZA0l3Agc2qLokIm7ub72IWAhMlvQaYL6k2yLihbzNfYDvAh+JiOe2ttOS5gBzAMaPH7+1q5uZWUXTIIiIqe08QET0StoAHAl0S9qDFALfiojvVZo+KemgiFgj6SDgqQG2OQ+YB1Cr1aKd/pmZla4jp4YkTcgXh5F0KHA48LAkAVcDvRHxj3Wr3QLMzvOzgX6PNszMbPtp9/bRWZJWAccCt0q6PVcdDyyRtBi4CTg3ItYBxwFnAidVbi99Z15nLjBN0nJgWl42M7MOU8TQPrNSq9Wiu7t7sLthZrbTkbQoImrN2vmbxWZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZla4toJA0umSeiRtllSrlB8jaXGelkiaVbfe7pLul/TDStkESQslLZd0vaQ92+mbmZm1pt0jgqXAKcC9DcprEXEUMB24SlJXpf7DQG/dOpcDV0bEROBp4Ow2+2ZmZi1oKwgiojciljUo3xgRm/LiMCD66iSNA/4Y+GqlTMBJwI25aD4ws52+mZlZazp2jUDSFEk9wAPAOZVg+DzwcWBzpfl+wDOVNquAgzvVNzMze1HTIJB0p6SlDaYZA60XEQsjYjLwBuBiScMkvQt4KiIW1T9Mo00M0Kc5krolda9du7bZEMzMbABdzRpExNR2HiAieiVtAI4EjgP+VNI7SaeMRki6BjgTGCmpKx8VjANWD7DNecA8gFqt1m9gmJlZcx05NZTvAOrK84cChwMPR8TFETEuIg4DzgD+X0S8LyICuBs4LW9iNnBzJ/pmZmZbavf20VmSVgHHArdKuj1XHQ8skbQYuAk4NyLWNdnchcD5klaQrhlc3U7fzMysNUo740NXrVaL7u7uwe6GmdlOR9KiiKg1a+dvFpuZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVrKwgknS6pR9JmSbVK+TGSFudpiaRZlbqRkm6U9GtJvZKOzeWjJd0haXn+O6qdvpmZWWvaPSJYCpwC3NugvBYRRwHTgaskdeW6LwA/iogjgNcBvbn8IuCuiJgI3JWXzcysw9oKgojojYhlDco3RsSmvDgMCABJI4A3A1fndr+LiGdyuxnA/Dw/H5jZTt/MzKw1HbtGIGmKpB7gAeCcHAyvANYCX5d0v6SvShqeVzkgItYA5L9jB9j2HEndkrrXrl3bqSGYmRWhaRBIulPS0gbTjIHWi4iFETEZeANwsaRhQBfwR8BXIuJoYAPbcAooIuZFRC0iamPGjNna1c3MrKKrWYOImNrOA0REr6QNwJHAKmBVRCzM1TfyYhA8KemgiFgj6SDgqXYe18zMWtORU0OSJvRdHJZ0KHA48HBEPAE8Junw3PRtwK/y/C3A7Dw/G7i5E30zM7MtNT0iGEi+LfSLwBjgVkmLI+IdwPHARZL+E9gMnBsR6/Jq/wP4lqQ9gZXAB3L5XOAGSWcDjwKnt9M3MzNrjSJisPvQllqtFt3d3YPdDTOznY6kRRFRa9bO3yw2Myucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwrUVBJJOl9QjabOkWqX8GEmL87RE0qxK3UfzOkslXStpWC6fIGmhpOWSrpe0Zzt9MzOz1rR7RLAUOAW4t0F5LSKOAqYDV0nqknQw8Je57khgd+CMvM7lwJURMRF4Gji7zb6ZmVkL2gqCiOiNiGUNyjdGxKa8OAyISnUX8DJJXcDewGpJAk4Cbsxt5gMz2+mbmZm1pmPXCCRNkdQDPACcExGbIuJx4HPAo8Aa4NmI+L/AfsAzlfBYBRzcqb6ZmdmLmgaBpDvz+fz6acZA60XEwoiYDLwBuFjSMEmjgBnABODlwHBJ7wPUaBMD9GmOpG5J3WvXrm02BDMzG0BXswYRMbWdB4iIXkkbgCNJAfBQRKwFkPQ94E3At4CRkrryUcE4YPUA25wHzAOo1Wr9BoaZmTXXkVND+Q6grjx/KHA48DDplNAbJe2drwu8DeiNiADuBk7Lm5gN3NyJvpmZ2ZbavX10lqRVwLHArZJuz1XHA0skLQZuAs6NiHURsZB0Qfg+0rWD3ch79sCFwPmSVpCuGVzdTt/MzKw1SjvjQ1etVovu7u7B7oaZ2U5H0qKIqDVr528Wm5kVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhWsrCCSdLqlH0mZJtQb14yWtl3RBpWy6pGWSVki6qFI+QdJCScslXS9pz3b6ZmZmrWn3iGApcApwbz/1VwK39S1I2h34MnAyMAl4t6RJufpy4MqImAg8DZzdZt/MzKwFbQVBRPRGxLJGdZJmAiuBnkrxMcCKiFgZEb8DrgNmSBJwEnBjbjcfmNlO38zMrDUduUYgaThwIXBZXdXBwGOV5VW5bD/gmYjYVFfe3/bnSOqW1L127drt13EzswI1DQJJd0pa2mCaMcBql5FO86yv31yDtjFAeUMRMS8iahFRGzNmTLMhmJnZALqaNYiIqduw3SnAaZKuAEYCmyW9ACwCDqm0GwesBtYBIyV15aOCvnIzM+uwpkGwLSLihL55SZcC6yPiS5K6gImSJgCPA2cA74mIkHQ3cBrpusFs4OZO9M3MzLbU7u2jsyStAo4FbpV0+0Dt897+h4DbgV7ghojou5h8IXC+pBWkawZXt9M3MzNrjSL6PRU/JNRqteju7h7sbpiZ7XQkLYqIl3zHq56/WWxmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVjgHgZlZ4RwEZmaFaysIJJ0uqUfSZkm1BvXjJa2XdEFePkTS3ZJ683ofrrQdLekOScvz31Ht9M3MzFrT7hHBUuAU4N5+6q8EbqssbwI+FhGvAd4InCdpUq67CLgrIiYCd+VlMzPrsLaCICJ6I2JZozpJM4GVQE+l/ZqIuC/PPw/0Agfn6hnA/Dw/H5jZTt/MzKw1HblGIGk4cCFw2QBtDgOOBhbmogMiYg2kwADGdqJvZma2pa5mDSTdCRzYoOqSiLi5n9UuA66MiPWSGm1zH+C7wEci4rmt6G/f+nOAOQDjx4/f2tXNzKyiaRBExNRt2O4U4DRJVwAjgc2SXoiIL0nagxQC34qI71XWeVLSQRGxRtJBwFMD9GkeMA+gVqvFNvTPzMyypkGwLSLihL55SZcC63MICLga6I2If6xb7RZgNjA3/+3vaMPMzLajdm8fnSVpFXAscKuk25uschxwJnCSpMV5emeumwtMk7QcmJaXzcyswxQxtM+s1Gq16O7uHuxumJntdCQtioiXfMernr9ZbGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFW7I/+icpLXAI4Pdj620P7BusDuxg5Q0VvB4d2VDcayHRsSYZo2GfBAMRZK6W/lFwF1BSWMFj3dXtiuP1aeGzMwK5yAwMyucg2BwzBvsDuxAJY0VPN5d2S47Vl8jMDMrnI8IzMwK5yDoEEmjJd0haXn+O6qfdrNzm+WSZjeov0XS0s73eNu1M1ZJe0u6VdKvJfVImrtje986SdMlLZO0QtJFDer3knR9rl8o6bBK3cW5fJmkd+zIfm+LbR2rpGmSFkl6IP89aUf3fVu089rm+vGS1ku6YEf1ebuKCE8dmIArgIvy/EXA5Q3ajAZW5r+j8vyoSv0pwLeBpYM9nk6NFdgbeGtusyfwY+DkwR5Tg/7vDvwGeEXu5xJgUl2bc4F/zvNnANfn+Um5/V7AhLyd3Qd7TB0a69HAy/P8kcDjgz2eTo63Uv9d4DvABYM9nm2ZfETQOTOA+Xl+PjCzQZt3AHdExL9HxNPAHcB0AEn7AOcDn9kBfW3XNo81IjZGxN0AEfE74D5g3A7o89Y6BlgREStzP68jjbuq+jzcCLxNknL5dRHx24h4CFiRt7ez2uaxRsT9EbE6l/cAwyTttUN6ve3aeW2RNJO0Y9Ozg/q73TkIOueAiFgDkP+ObdDmYOCxyvKqXAbwaeAfgI2d7OR20u5YAZA0EvgT4K4O9bMdTftfbRMRm4Bngf1aXHdn0s5Yq04F7o+I33aon9vLNo9X0nDgQuCyHdDPjuka7A4MZZLuBA5sUHVJq5toUBaSjgJeFREfrT8XOVg6NdbK9ruAa4F/ioiVW9/Djhuw/03atLLuzqSdsaZKaTJwOfD27divTmlnvJcBV0bE+nyAMCQ5CNoQEVP7q5P0pKSDImKNpIOApxo0WwWcWFkeBywAjgVeL+lh0ms0VtKCiDiRQdLBsfaZByyPiM9vh+52wirgkMryOGB1P21W5WDbF/j3FtfdmbQzViSNA24CzoqI33S+u21rZ7xTgNMkXQGMBDZLeiEivtT5bm9Hg32RYledgM+y5QXUKxq0GQ08RLpoOirPj65rcxg7/8XitsZKug7yXWC3wR7LAGPsIp0HnsCLFxQn17U5jy0vKN6Q5yez5cXilezcF4vbGevI3P7UwR7HjhhvXZtLGaIXiwe9A7vqRDpfehewPP/t+9CrAV+ttPsz0sXDFcAHGmxnKATBNo+VtPcVQC+wOE8fHOwx9TPOdwIPku4wuSSXfQr40zw/jHTnyArg58ArKutektdbxk54V9T2Givw18CGymu5GBg72OPp5Gtb2caQDQJ/s9jMrHC+a8jMrHAOAjOzwjkIzMwK5yAwMyucg8DMrHAOAjOzwjkIzMwK5yAwMyvc/wdOBskt4fWQ7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [-1354.7520731954903]\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while ppo_update_idx < max_ppo_updates and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "    state = envs.reset()\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "              \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "    \n",
    "    ppo_update_idx += 1\n",
    "    if ppo_update_idx % 20 == 0:\n",
    "        test_reward = np.mean([test_env() for _ in range(100)])\n",
    "        test_rewards.append(test_reward)\n",
    "        plot(ppo_update_idx, test_rewards)\n",
    "        print('Rewards:', test_rewards)\n",
    "        if test_reward > threshold_reward: early_stop = True\n",
    "        if test_reward > highest_test_reward:\n",
    "            torch.save(model.state_dict(), 'saved_networks/D20T200_network')\n",
    "            highest_test_reward = test_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "#torch.save(model.state_dict(), 'saved_networks/ppo_network')\n",
    "\n",
    "\n",
    "# Load:\n",
    "#model.load_state_dict(torch.load('saved_networks/ppo_network'))\n",
    "#model.load_state_dict(torch.load('saved_networks/ppo_network_IC_uni_pm0dot5_v_0dot5_to_10_uni_3x128_nodes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get average cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward = np.mean([test_env() for _ in range(200)])\n",
    "test_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = np.array([np.deg2rad(-5), 0, 9], dtype=np.float32)\n",
    "nr_steps_to_render = 30\n",
    "test_env(vis=True, init_state=init_state, nr_steps=nr_steps_to_render)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sequnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "changing_speed = False\n",
    "init_state = np.array([np.deg2rad(-0.5), 0, 0.5],dtype=np.float32)\n",
    "state_0 = env.reset(init_state=init_state, changing_speed  = changing_speed)\n",
    "phi_sequence, delta_sequence, velocity = get_phi_sequence(state_0)\n",
    "\n",
    "Ts = 0.04\n",
    "t = np.arange(0, len(phi_sequence)*Ts, Ts)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14,8))\n",
    "\n",
    "\n",
    "axes[0].plot(t, np.rad2deg(phi_sequence), label = \"RL controller\")\n",
    "axes[0].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[0].set_ylabel(r'$\\varphi$ (degrees)', fontsize = 16)\n",
    "axes[0].set_title(r'Roll angle $\\varphi$. Speed: {:.2f} m/s'.format(state_0[2]), fontsize=16)\n",
    "axes[0].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[0].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "\n",
    "axes[1].plot(t[1:], np.rad2deg(delta_sequence), label=\"RL controller\")\n",
    "axes[1].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[1].set_ylabel('$\\delta$ (degrees)', fontsize = 16)\n",
    "axes[1].set_title('Steering angle $\\delta$. Speed: {:.2f} m/s'.format(state_0[2]), fontsize = 16);\n",
    "axes[1].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[1].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "\n",
    "axes[2].plot(t, velocity)\n",
    "axes[2].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[2].set_ylabel('Speed (m/s)', fontsize = 16)\n",
    "axes[2].set_title('Speed: {:.2f} m/s'.format(state_0[2]), fontsize = 16);\n",
    "axes[2].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[2].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "# Plotting the optimal sequences as well (they come from using the K gain from the dlqr algorithm)\n",
    "from utility_functions import get_optimal_sequence\n",
    "optimal_phi_sequence, optimal_delta_sequence = get_optimal_sequence(init_state, env, changing_speed = changing_speed)\n",
    "t_optimal = np.arange(0, len(optimal_phi_sequence)*Ts, Ts)\n",
    "\n",
    "axes[0].plot(t_optimal, np.rad2deg(optimal_phi_sequence), alpha=0.7, label = \"LQR controller\")\n",
    "axes[0].legend(fontsize=14)\n",
    "axes[1].plot(t_optimal[1:], np.rad2deg(optimal_delta_sequence), alpha=0.7, label=\"LQR controller\")\n",
    "axes[1].legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
