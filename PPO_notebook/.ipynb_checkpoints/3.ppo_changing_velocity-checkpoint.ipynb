{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 64\n",
    "env_name = \"BikeLQR_4states-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_size, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_size, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    #clear_output(False)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('update %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False, init_state=None, nr_steps=int(1e9)):\n",
    "    if type(init_state) == np.ndarray:\n",
    "        state = env.reset(init_state=init_state)\n",
    "    else:\n",
    "        state = env.reset()\n",
    "        \n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for i in range(nr_steps):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        \n",
    "        if vis:\n",
    "            time.sleep(0.1)\n",
    "            env.render() \n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def get_phi_sequence(state_0):\n",
    "    state = state_0.copy()\n",
    "    phi_sequence = [state[0]]\n",
    "    delta_sequence = []\n",
    "    v_sequence = [state[2]]\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        phi_sequence.append(next_state[0])\n",
    "        delta_sequence.append(action)\n",
    "        v_sequence.append(next_state[2])\n",
    "        state = next_state.copy()\n",
    "    return phi_sequence, delta_sequence, v_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 32\n",
    "lr               = 1e-5\n",
    "#lr               = 1e-6\n",
    "num_steps        = 60\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 10000000\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ppo_updates = 40000\n",
    "ppo_update_idx  = 0\n",
    "test_rewards = []\n",
    "highest_test_reward = -np.inf\n",
    "#highest_test_reward = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('saved_networks/ppo_network_IC_uni_pm5_v_0dot5_to_10_uni_eplen100_lr9e-6_brute_3x128_nodes_ver3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [-1349.199523566876]\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while ppo_update_idx < max_ppo_updates and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "    state = envs.reset()\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "              \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "    \n",
    "    ppo_update_idx += 1\n",
    "    if ppo_update_idx % 20 == 0:\n",
    "        test_reward = np.mean([test_env() for _ in range(100)])\n",
    "        test_rewards.append(test_reward)\n",
    "        plot(ppo_update_idx, test_rewards)\n",
    "        print('Rewards:', test_rewards)\n",
    "        if test_reward > threshold_reward: early_stop = True\n",
    "        if test_reward > highest_test_reward:\n",
    "            torch.save(model.state_dict(), 'saved_networks/best_network')\n",
    "            highest_test_reward = test_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "#torch.save(model.state_dict(), 'saved_networks/ppo_network')\n",
    "\n",
    "\n",
    "# Load:\n",
    "#model.load_state_dict(torch.load('saved_networks/ppo_network'))\n",
    "#model.load_state_dict(torch.load('saved_networks/ppo_network_IC_uni_pm0dot5_v_0dot5_to_10_uni_3x128_nodes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get average cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward = np.mean([test_env() for _ in range(200)])\n",
    "test_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = np.array([np.deg2rad(-5), 0, 9], dtype=np.float32)\n",
    "nr_steps_to_render = 30\n",
    "test_env(vis=True, init_state=init_state, nr_steps=nr_steps_to_render)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sequnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "changing_speed = False\n",
    "init_state = np.array([np.deg2rad(-0.5), 0, 0.5],dtype=np.float32)\n",
    "state_0 = env.reset(init_state=init_state, changing_speed  = changing_speed)\n",
    "phi_sequence, delta_sequence, velocity = get_phi_sequence(state_0)\n",
    "\n",
    "Ts = 0.04\n",
    "t = np.arange(0, len(phi_sequence)*Ts, Ts)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14,8))\n",
    "\n",
    "\n",
    "axes[0].plot(t, np.rad2deg(phi_sequence), label = \"RL controller\")\n",
    "axes[0].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[0].set_ylabel(r'$\\varphi$ (degrees)', fontsize = 16)\n",
    "axes[0].set_title(r'Roll angle $\\varphi$. Speed: {:.2f} m/s'.format(state_0[2]), fontsize=16)\n",
    "axes[0].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[0].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "\n",
    "axes[1].plot(t[1:], np.rad2deg(delta_sequence), label=\"RL controller\")\n",
    "axes[1].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[1].set_ylabel('$\\delta$ (degrees)', fontsize = 16)\n",
    "axes[1].set_title('Steering angle $\\delta$. Speed: {:.2f} m/s'.format(state_0[2]), fontsize = 16);\n",
    "axes[1].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[1].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "\n",
    "axes[2].plot(t, velocity)\n",
    "axes[2].set_xlabel('Time (s)', fontsize = 16)\n",
    "axes[2].set_ylabel('Speed (m/s)', fontsize = 16)\n",
    "axes[2].set_title('Speed: {:.2f} m/s'.format(state_0[2]), fontsize = 16);\n",
    "axes[2].tick_params(axis=\"x\", labelsize=14)\n",
    "axes[2].tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "# Plotting the optimal sequences as well (they come from using the K gain from the dlqr algorithm)\n",
    "from utility_functions import get_optimal_sequence\n",
    "optimal_phi_sequence, optimal_delta_sequence = get_optimal_sequence(init_state, env, changing_speed = changing_speed)\n",
    "t_optimal = np.arange(0, len(optimal_phi_sequence)*Ts, Ts)\n",
    "\n",
    "axes[0].plot(t_optimal, np.rad2deg(optimal_phi_sequence), alpha=0.7, label = \"LQR controller\")\n",
    "axes[0].legend(fontsize=14)\n",
    "axes[1].plot(t_optimal[1:], np.rad2deg(optimal_delta_sequence), alpha=0.7, label=\"LQR controller\")\n",
    "axes[1].legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
